{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bcf6cf-bee4-4979-b695-9c7ed0f97186",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### First i install all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b823cfa-3428-497f-bce4-908eb1ebebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai-whisper\n",
    "!pip install yt_dlp\n",
    "!pip install unsloth[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd6547-cfe6-4023-8669-3378fef2e5da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import all the packages here which are necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cff21c-1c22-45fd-b699-11c0fbf1064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ,json,subprocess,uuid ,torch,whisper,glob,yt_dlp,io\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec492111-78b0-48a6-a4bb-d798d3dc6da2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 1: Select the audio file from web or local or from anywhere ,\n",
    "#### I  download the file from youtube this will dowload the file and save in your current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04abab-7189-452e-9e1f-c6cd7d332311",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio_dir = Path(\"/content/audio\")\n",
    "Audio_dir.mkdir(exist_ok=True)\n",
    "url = ['https://www.youtube.com/watch?v=mKBbP4T5fbk&t=142s&ab_channel=SpeakEnglishWithTiffani']\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'extractaudio': True,\n",
    "    'audioformat': 'm4a',\n",
    "     'outtmpl': os.path.join(Audio_dir, '%(title)s.%(ext)s')\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca6acf-5c6b-438e-9a98-3c16633a9b40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Here I check the file duration  using (Subprocess class) and using (ffprobe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03285a-5483-4ee1-ade2-4b009e9341e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_sec(path:Path)->float:\n",
    "  # return seconds using ffprob\n",
    "  pr  = subprocess.run(['ffprobe','-v','error','-show_entries','format=duration','-of','json',str(path)],capture_output=True,text=True)\n",
    "  try :\n",
    "    return float(json.loads(pr.stdout)[\"format\"][\"duration\"])\n",
    "  except (json.JSONDecodeError,ValueError,KeyError) as e:\n",
    "    print(f\"Error getting duration for {path}:{e}\")\n",
    "    return 0.0\n",
    "\n",
    "audio_paths = sorted(chain(\n",
    "    Path('/content/audio').glob('*.m4a'),\n",
    "    Path('/content/audio').glob('*.mp3'),\n",
    "    Path('/content/audio').glob('*.wav'),\n",
    "    Path('/content/audio').glob('*.webm'),\n",
    "    ))\n",
    "print(f\" Found {len(audio_paths)} audio files: \")\n",
    "\n",
    "total_duration = 0\n",
    "for files in audio_paths:\n",
    "  dur_sec = get_duration_sec(files)\n",
    "  total_duration += dur_sec\n",
    "  print(f\" {files.name:<40} : {dur_sec/60:6.2f} min\")\n",
    "\n",
    "print(f\"Total duration: {total_duration/60:6.2f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac314454-a6fe-4fb6-8b7f-be08a9a9b7d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 2 : Cleaning the Voice using (ffmpeg and its some filters) and save in a directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204c8d6-b89b-4454-9777-505365ba8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_Audio_dir = Path('/content/clean_audio')\n",
    "clean_Audio_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def clean_audio(src:Path)->Path:\n",
    "  #FFmpeg filters to clean and normalise audio\n",
    "  out = clean_Audio_dir/f'{src.stem}_clean.wav'\n",
    "  filters = (\n",
    "      \"highpass=f=90,\"                # this is use to remove low frequency noise below 90hz\n",
    "      \"afftdn,\"                       # this is FFT based denoiser to reduce broadband noise\n",
    "      \"loudnorm=I=-16:LRA=11:TP=-1.5,\" # Apply loudness normalization\n",
    "      \"dynaudnorm=f=200,\"             #dynamic audio normalization to adjust volume based on given frame size\n",
    "      \"apad=pad_dur=0.1\"             # add 100ms of silence padding at the end of the video\n",
    "  )\n",
    "  subprocess.run([\"ffmpeg\",\"-y\",\"-i\",str(src),\n",
    "                  \"-af\",filters,\"-ar\",\"32000\",\"-ac\",\"1\",str(out)],check=True)#sample rate 40000 and output as mono channel\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c0d07-93d5-4878-8caf-c003489321fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cleaned= []\n",
    "total_duration_cleaned = 0\n",
    "for files in audio_paths:\n",
    "  clean_file = Path(clean_Audio_dir/f'{files.stem}_clean.wav')\n",
    "  if clean_file.exists():\n",
    "    print(f\"Skipping {clean_file.name} file already Exist \")\n",
    "    total_duration_cleaned += get_duration_sec(clean_file)\n",
    "    total_cleaned.append(clean_file)\n",
    "  else:\n",
    "    print(f'Cleaning {files.name} - ',end=\"\")\n",
    "    total_duration_cleaned += get_duration_sec(files)\n",
    "    total_cleaned.append(clean_audio(files))\n",
    "    print(\"Done \",)\n",
    "\n",
    "print(f\"Total duration: {total_duration/60:6.2f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb58b43-8381-4368-b2c9-fab7fe68d5ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 3 : Now we are Transcribing(means convert the audio to text) the audio using whiper library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d46c5-e423-4e8b-8978-8d5229b64d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = \"large\"  #@param [\"tiny\", \"base\", \"small\", \"medium\", \"large\"]\n",
    "DEVICE = 'cuda'        #\"cuda\" tells PyTorch to use an NVIDIA GPU if available.\n",
    "                      #\"cpu\" means run on the CPU (slower).\n",
    "model = whisper.load_model(MODEL_SIZE,device=DEVICE)\n",
    "Trancripts_dir = Path('/content/trancripts')\n",
    "Trancripts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for audio_path in total_cleaned:\n",
    "  out_json = Trancripts_dir/f'{audio_path.stem}.json'\n",
    "\n",
    "  if out_json.exists():\n",
    "    print(f\"Skipping {out_json.name} file already Exist \")\n",
    "    continue\n",
    "\n",
    "  print(f\"Transcribing - {out_json.stem}..\")\n",
    "\n",
    "  result = model.transcribe(str(audio_path),word_timestamps=True,\n",
    "                            fp16=(DEVICE=='cuda'),verbose=True)\n",
    "  with out_json.open('w') as f:\n",
    "    json.dump(result,f,indent=2)\n",
    "\n",
    "  print(f\"Saved {out_json.name} in Transcripts Directory \")\n",
    "print(\"All transcried are Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dfa165-7738-4071-ab30-b79db7016912",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 4 : Now we are making the dataset using audio and transcribe file for train the model on our downloaded audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d5109-2713-4e4c-99e1-c6ff6ebb6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_audio_text_dataset(json_file_path, audio_file_path, output_csv_path, audio_chunks_dir,chunk_duration=30):\n",
    "    \n",
    "    # Create output directory for audio segments\n",
    "    os.makedirs(audio_chunks_dir, exist_ok=True)\n",
    "    \n",
    "    # Read the JSON file\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Try to load audio processing libraries\n",
    "  \n",
    "    print(\"Using pydub for audio processing...\")\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "    sample_rate = audio.frame_rate\n",
    "    print(f\"Audio loaded successfully: {len(audio)/1000:.2f} seconds at {sample_rate} Hz\")\n",
    "    total_duration = len(audio) / 1000\n",
    "\n",
    "    \n",
    "    # Extract segments and create dataset rows\n",
    "    segments = data.get('segments', [])\n",
    "    \n",
    "    rows = []\n",
    "    chunk_id = 0\n",
    "    current_chunk_start = 0.0\n",
    "    current_chunk_texts = []\n",
    "    current_chunk_end = 0.0\n",
    "        \n",
    "    for segment in segments:\n",
    "        seg_start = segment.get('start', 0.0)\n",
    "        seg_end = segment.get('end', 0.0)\n",
    "        seg_text = segment.get('text', '').strip()\n",
    "            \n",
    "        if not seg_text:\n",
    "                continue\n",
    "            \n",
    "        # Check if adding this segment would exceed 30 seconds\n",
    "        potential_chunk_duration = seg_end - current_chunk_start\n",
    "            \n",
    "        # If this would make chunk too long (>30s), save current chunk and start new one\n",
    "        if potential_chunk_duration > 30.0 and current_chunk_texts:\n",
    "             # Save current chunk\n",
    "            try:\n",
    "                # Extract audio for current chunk\n",
    "                start_ms = int(current_chunk_start * 1000)\n",
    "                end_ms = int(current_chunk_end * 1000)\n",
    "                    \n",
    "                audio_chunk = audio[start_ms:end_ms]\n",
    "                    \n",
    "                # Save audio chunk to file\n",
    "                audio_filename = f\"chunks_{chunk_id:03d}.wav\"\n",
    "                audio_file_path = os.path.join(audio_chunks_dir, audio_filename)\n",
    "                audio_chunk.export(audio_file_path, format=\"wav\")\n",
    "              \n",
    "                combined_text = ' '.join(current_chunk_texts).strip()\n",
    "                    \n",
    "                # Create row with perfectly aligned audio and text\n",
    "                audio_with_path = {\n",
    "                    'path':  audio_file_path\n",
    "                }\n",
    "                    \n",
    "                row = {\n",
    "                    'audio': audio_with_path,\n",
    "                    'text': combined_text,\n",
    "                    'source': \"0\"\n",
    "                }\n",
    "                rows.append(row)\n",
    "                    \n",
    "                chunk_duration_actual = current_chunk_end - current_chunk_start\n",
    "                print(f\"Chunk {chunk_id}: {current_chunk_start:.1f}s-{current_chunk_end:.1f}s ({chunk_duration_actual:.1f}s), {len(combined_text)} chars\")\n",
    "                chunk_id += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk {chunk_id}: {e}\")\n",
    "                \n",
    "                # Start new chunk with current segment\n",
    "            current_chunk_start = seg_start\n",
    "            current_chunk_texts = [seg_text]\n",
    "            current_chunk_end = seg_end\n",
    "        else:\n",
    "            # Add this segment to current chunk\n",
    "            if not current_chunk_texts:  # First segment in chunk\n",
    "                current_chunk_start = seg_start\n",
    "            current_chunk_texts.append(seg_text)\n",
    "            current_chunk_end = seg_end\n",
    "        \n",
    "        # Don't forget the last chunk\n",
    "    if current_chunk_texts:\n",
    "        try:\n",
    "            start_ms = int(current_chunk_start * 1000)\n",
    "            end_ms = int(current_chunk_end * 1000)\n",
    "                \n",
    "            audio_chunk = audio[start_ms:end_ms]\n",
    "                \n",
    "            # Save audio chunk to file\n",
    "            audio_filename = f\"chunks_{chunk_id:03d}.wav\"\n",
    "            audio_file_path = os.path.join(audio_chunks_dir, audio_filename)\n",
    "            audio_chunk.export(audio_file_path, format=\"wav\")\n",
    "                \n",
    "            combined_text = ' '.join(current_chunk_texts).strip()\n",
    "                \n",
    "            audio_with_path = {\n",
    "                'path': audio_file_path\n",
    "            }\n",
    "                \n",
    "            row = {\n",
    "                'audio': audio_with_path,\n",
    "                'text': combined_text,\n",
    "                'source': \"0\"\n",
    "            }\n",
    "            rows.append(row)\n",
    "                \n",
    "            chunk_duration_actual = current_chunk_end - current_chunk_start\n",
    "            print(f\"Chunk {chunk_id}: {current_chunk_start:.1f}s-{current_chunk_end:.1f}s ({chunk_duration_actual:.1f}s), {len(combined_text)} chars\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing final chunk: {e}\")\n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_json(output_csv_path, index=False,orient=\"records\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATASET CREATED SUCCESSFULLY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\" Dataset file: {output_csv_path}\")\n",
    "    print(f\" Audio segments folder: {audio_chunks_dir}\")\n",
    "    print(df.head().to_string())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83dc86e-f4a6-44ed-80f3-19827e66ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = r\"/content/trancripts/ENGLISH MASTERCLASS ｜ THINK & SPEAK ENGLISH [FULL LESSON]_clean.json\"\n",
    "audio_file = r\"/content/clean_audio/ENGLISH MASTERCLASS ｜ THINK & SPEAK ENGLISH [FULL LESSON]_clean.wav\"\n",
    "dataset_json = r\"/content/our_dataset/dataset_json.json\"\n",
    "audio_segments_dir = r\"/content/myaudio_segments\"\n",
    "    \n",
    "our_dataset = Path(\"/content/our_dataset\")\n",
    "our_dataset.mkdir(exist_ok = True)\n",
    "# Check if files exist\n",
    "if not os.path.exists(json_file):\n",
    "    print(f\"JSON file not found: {json_file}\")\n",
    "    exit(1)\n",
    "    \n",
    "if not os.path.exists(audio_file):\n",
    "    print(f\"Audio file not found: {audio_file}\")\n",
    "    print(\"Please ensure the audio file is in the Downloads folder\")\n",
    "    exit(1)\n",
    "    \n",
    "print(\"Starting dataset creation...\")\n",
    "main_df = create_audio_text_dataset(json_file, audio_file,dataset_json, audio_segments_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213c5da1-09cb-4159-9c4d-ff959cd24f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef13c0-c8b5-4621-ac3c-9e63b0b8a7e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 5 : Gathering the unsloth/csm-1b model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a745e-7e08-41a2-a843-2f459aa8a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gatting the predefined model of unsloth \n",
    "from unsloth import FastModel\n",
    "from transformers import CsmForConditionalGeneration\n",
    "\n",
    "model_name = \"unsloth/csm-1b\"\n",
    "\n",
    "model,processor = FastModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    auto_model = CsmForConditionalGeneration,\n",
    "    load_in_4bit = False,\n",
    "    full_finetuning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765ea35-e933-4359-b7fd-439ff04abe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bb2aa-f352-4c53-ae4d-febd254729a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = FastModel.get_peft_model(\n",
    "     model,\n",
    "     r= 32,\n",
    "     target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\n",
    "                       \"o_proj\",\"gate_proj\",\"up_proj\",\n",
    "                       \"down_proj\",],\n",
    "     lora_alpha = 16,\n",
    "     lora_dropout = 0,\n",
    "     bias = \"none\",\n",
    "     use_gradient_checkpointing = \"unsloth\",\n",
    "     randomstate = 3407,\n",
    "     use_rslora = True,\n",
    "     loftq_config = None,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291fe88-4da6-42b3-83b0-4912cd49f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "run_name =model_name.split(\"/\")[-1]+'-lora-ft'+time.strftime(\"_%Y%m%d_%H%M%S\")\n",
    "print(run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4182e7-88a2-48f1-866f-08ee81f4b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e132e-16c4-49f2-ad3c-4315e89699e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 6: Get the dataset which you made for split the data into train and test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e50a1-d1d6-4288-89e4-2b9504bb0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 24000\n",
    "\n",
    "import os\n",
    "from transformers import AutoProcessor\n",
    "from datasets import load_dataset,Audio\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"unsloth/csm-1b\")\n",
    "# raw_ds = load_dataset(exa_dataset,split=\"train\")\n",
    "\n",
    "dataset = load_dataset(\"json\",data_files=\"/content/our_dataset/dataset_json.json\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio())\n",
    "raw_ds = dataset[\"train\"]\n",
    "\n",
    "print(f'Dataset loaded with features: {raw_ds.features[\"audio\"]}')\n",
    "\n",
    "if \"source\" not in raw_ds.column_names:\n",
    "   print(\"Unsloth No speaker Found\")\n",
    "   new_column = [\"0\"]*len(raw_ds)\n",
    "   raw_ds = raw_ds.add_column(name=\"source\",column=new_column)\n",
    "elif \"source\" in raw_ds.column_names:\n",
    "   speaker_key = \"source\"\n",
    "\n",
    "target_sampling_rate = sample_rate\n",
    "raw_ds = raw_ds.cast_column(\"audio\",Audio(sampling_rate=target_sampling_rate))\n",
    "\n",
    "import math\n",
    "from datasets import DatasetDict\n",
    "\n",
    "totalrows = len(raw_ds)\n",
    "test_rows = min(30,max(1,math.ceil(0.10*totalrows)))\n",
    "\n",
    "split:DatasetDict = raw_ds.train_test_split(test_size=test_rows,shuffle=True,seed=42)\n",
    "\n",
    "rawTrainDs = split[\"train\"]\n",
    "rawTestDs = split[\"test\"]\n",
    "\n",
    "print(f\"Train rows: {len(rawTrainDs)}\")\n",
    "print(f\"Eval rows: {len(rawTestDs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30989f1a-8e40-4996-bf86-87cdb8c8f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_audio_length = 0\n",
    "max_text_length = 0 # Initialize max_text_length\n",
    "for row in raw_ds:\n",
    "    text_length = len(row['text'])\n",
    "    audio_length = len(row['audio']['array'])\n",
    "    max_text_length = max(max_text_length, text_length)\n",
    "    max_audio_length = max(max_audio_length, audio_length)\n",
    "\n",
    "print(f\"Maximum text length: {max_text_length}\")\n",
    "print(f\"Maximum audio length: {max_audio_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103abef-7723-4273-9fb0-dbeaa9a29c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 7 :  Now prepocess the data or cleaning the dataset and structure the dataset to understand by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a15f1b-4bbd-4176-bcee-979b2cc68e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "  conversation = [\n",
    "      {\n",
    "          \"role\": str(example[\"source\"]),\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\", \"text\": example[\"text\"]},\n",
    "              {\"type\": \"audio\", \"audio\": example[\"audio\"][\"array\"]} # Pass the numpy array\n",
    "          ],\n",
    "      }\n",
    "  ]\n",
    "  try :\n",
    "    model_inputs = processor.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=True,\n",
    "        return_dict =True,\n",
    "        output_labels = True,\n",
    "        text_kwargs={\n",
    "            \"padding\":\"max_length\",\n",
    "            \"max_length\":max_text_length,\n",
    "            \"pad_to_multiple_of\":8,\n",
    "            \"padding_side\":\"right\",\n",
    "        },\n",
    "        audio_kwargs={\n",
    "            \"sampling_rate\":24000,\n",
    "            \"padding\":\"max_length\",\n",
    "            \"max_length\":max_audio_length,\n",
    "        },\n",
    "        common_kwargs = {\"return_tensors\":\"pt\"}\n",
    "    )\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return None\n",
    "  required_keys = [\"input_ids\",\"attention_mask\",\"labels\",\"input_values\",\"input_values_cutoffs\"] # Corrected typo here\n",
    "  processed = {}\n",
    "\n",
    "  for key in required_keys:\n",
    "    if key not in model_inputs:\n",
    "      print(f\"{key} missing from model_inputs\")\n",
    "      return None\n",
    "    value = model_inputs[key][0]\n",
    "    processed[key] = value\n",
    "\n",
    "  if not all(isinstance(processed[key],torch.Tensor) for key in processed):\n",
    "    print(\"Not all values in processed are tensors\")\n",
    "    return None\n",
    "  return processed\n",
    "\n",
    "processed_train_ds = rawTrainDs.map(preprocess,remove_columns=rawTrainDs.column_names,desc=\"Preprocessing Train data\")\n",
    "processed_test_ds = rawTestDs.map(preprocess,remove_columns=rawTestDs.column_names,desc=\"Preprocessing Test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f748f7-9727-4ccc-80f1-40d0ef7c5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a756f-c83f-4e5c-9ddc-c24e392ec5a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 8: We can train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f307a-6a2f-4098-9941-5e01ff9ebc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer=Trainer(\n",
    "    model =model,\n",
    "    train_dataset = processed_train_ds,\n",
    "    eval_dataset = processed_test_ds,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size =1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs =3,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 0.2,\n",
    "        learning_rate = 2e-4, #for lora,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps =1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"constant\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = 'tensorboard',\n",
    "        logging_dir = f\"logs/{run_name}\"\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35017e0-9746-4981-8060-bcb2b28e9c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19fbefa-473c-40e0-8239-e63c20c50595",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 9 : Now we Do Fine-tuning to check the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab93c5-0e96-4eab-a6ee-3bbc753d1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from IPython.display import Audio, display  \n",
    "text = \"\"\"Resemble AI can clone a voice with just a few minutes of recorded data, making it possible to \n",
    "generate realistic speech that closely matches the original speaker’s tone, style, and personality\n",
    ".Learning a new skill is like planting a seed; with patience, effort, and care, it will grow into something meaningful and rewarding over time.\n",
    "In today’s fast-paced digital world, communication has become more instant than ever, but the value of genuine human connection remains timeless.\n",
    "Technology is changing the way we live, work, and communicate with each other.\n",
    "Artificial intelligence is not here to replace humans, but to help us work smarter and faster.\n",
    "Reading books can transport you into different worlds and expand your imagination.\n",
    "\"\"\"\n",
    "speaker_id = 0\n",
    "inputs = processor(\n",
    "    f\"[{speaker_id}]{text}\",\n",
    "    add_special_tokens = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "audio_values = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=375,#125 tokens is 10 seconds of audio ,for longer speech increase this \n",
    "    #play with below parameters for better output\n",
    "    depth_decoder_temperature = 0.6,\n",
    "    depth_decoder_top_k = 0,\n",
    "    depth_decoder_top_p = 0.9,\n",
    "    temperature = 0.8,\n",
    "    top_k = 50,\n",
    "    top_p = 1.0,\n",
    "    output_audio = True\n",
    ")\n",
    "\n",
    "audio = audio_values[0].to(torch.float32).cpu().numpy()\n",
    "sf.write(\"finetuned.wav\",audio ,sample_rate)\n",
    "print(\"Fine-tuned:\")\n",
    "display(Audio(audio, rate=sample_rate))\n",
    "print(\"Real: \")\n",
    "display(Audio(rawTestDs[3][\"audio\"][\"array\"],rate =sample_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cccb298-d14c-4b49-8149-54ef6b2aab30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Step 10 : Now we Clone the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f250adc7-6380-4d3c-8149-2cf02c843014",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = 0\n",
    "\n",
    "cloned = rawTestDs[1][\"audio\"][\"array\"] \n",
    "cloned_text = rawTestDs[1][\"text\"]\n",
    "conversation = [\n",
    "{\"role\": str(speaker_id),\"content\": [{\"type\": \"text\", \"text\": cloned_text},{\"type\": \"audio\", \"audio\": cloned}],}\n",
    "  ,{\"role\": str(speaker_id),\"content\": [{\"type\": \"text\", \"text\": text}],}\n",
    "                 ]\n",
    "inputs = processor.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=True,\n",
    "        return_dict =True,\n",
    "        return_tensors='pt')\n",
    "\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "audio_values = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=375,#125 tokens is 10 seconds of audio ,for longer speech increase this ~12.5 tokens ≈ 1 second of audio\n",
    "    #play with below parameters for better output\n",
    "    depth_decoder_temperature = 0.6,\n",
    "    depth_decoder_top_k = 0,\n",
    "    depth_decoder_top_p = 0.9,\n",
    "    temperature = 0.8,\n",
    "    top_k = 50,\n",
    "    top_p = 1.0,\n",
    "    output_audio = True\n",
    ")\n",
    "\n",
    "audio = audio_values[0].to(torch.float32).cpu().numpy()\n",
    "sf.write(\"clonedVoice.wav\",audio ,sample_rate)\n",
    "print(\"cloned\")\n",
    "display(Audio(audio, rate=sample_rate))\n",
    "print(\"Real: \")\n",
    "display(Audio(rawTestDs[3][\"audio\"][\"array\"],rate =sample_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
